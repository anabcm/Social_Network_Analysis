{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de redes para directorio de servidores públicos del POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#Contruyendo la estructura de la red\n",
    "#Este codigo explora la estructura y directorio de los datos de transparencia que se encuentran en el POT\n",
    "#publicados por INAI en abril del 2016\n",
    "#El código interactua con una base de datos en postgres, las consultas son basicas en SQL\n",
    "#asi que es posible conectar otra base y sería necesario configurar el conector adecuado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ESPECIFICACIONES\n",
    "#Python 2 y 3\n",
    "#Base de datos en Postgres\n",
    "    #database=\"dir\"\n",
    "    #user=\"postgres\"\n",
    "    #password=\"postgres\"\n",
    "#Crear una carpeta llamada \"data\" para almacenar la salida de las dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LIBRERIAS Y PAQUETES\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "import json\n",
    "import csv\n",
    "import psycopg2\n",
    "import networkx as nx\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "from igraph import *\n",
    "#A la base de datos se integraron datos de SEP e INE como atributos para cada nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====INicio del programa====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Se pueden decomentar las funciones que se desean ejecutar\n",
    "def start():\n",
    "    database=\"dir\"\n",
    "    user=\"postgres\"\n",
    "    password=\"postgres\"\n",
    "    con=connect_database(database,user,password)\n",
    "    #explorando_red(con)    \n",
    "    #cleaning_data_estados(con) #limpia los datos de los estados y los integra\n",
    "    #archivo=\"infoteq.csv\"   #en caso de tener una lista de ids, agregar\n",
    "    #develop_net_dependencia(archivo,con) #si se quiere construir la red por archivo\n",
    "    net_by_dependecia(con)    #genera las redes para todas las dependencias y calcula las medidas\n",
    "start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=========conecta a una base de datos, regresa el cursor========================\n",
    "def connect_database(dbname,u,p):\n",
    "    con=None\n",
    "    try:\n",
    "        con=psycopg2.connect(database=dbname, user=u, password=u)\n",
    "        return con    \n",
    "    except psycopg2.DatabaseError, e:\n",
    "        print ('Error %s' % e    )\n",
    "        sys.exit(1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==================Contruye red por depenendecia con file======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#costruye la red de datos\n",
    "#Consulta la tabla estructura y construye la red basado en la jerarquia que ocupa\n",
    "#cada servidor publico\n",
    "def explorando_red(con):\n",
    "    f=open(\"output_redes.csv\",'wb')\n",
    "    cursor=con.cursor()\n",
    "    #consulta para traer las diferentes dependencias\n",
    "    query=\"select id, id_cargos,id_dependencia from dir_clean order by id_dependencia ASC\"    \n",
    "    cursor.execute(query)\n",
    "    rows=cursor.fetchall()\n",
    "    #escribe en el archibo para cada servidor y su cargo superior\n",
    "    f.write(\"id,id_dependecia,id_cargos,superiores\")\n",
    "    for row in rows:   \n",
    "        lista=[]\n",
    "        lista=visited_node(row[1],row[2],lista,con)\n",
    "        f.write(\"\\n\"+str(row[0])+\",\"+row[2]+\",\"+row[1]+\",\")\n",
    "        lista=lista[:len(lista)-1]\n",
    "        c=0        \n",
    "        for i in lista:\n",
    "            c=c+1\n",
    "            if c<len(lista):\n",
    "                f.write(str(i)+\",\")\n",
    "            else:\n",
    "                f.write(str(i))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================visita cada nodo y busca el superior=======================                \n",
    "#funcion recursiva que trae los id de los puestos superiores, con base en la tabla estrucura                \n",
    "def visited_node(id,dependencia,lista,con):  \n",
    "    cursor=con.cursor()    \n",
    "    query=\"select id_cargo_superior from estructura where id_cargos like '\"+str(id)+\"' and id_dependencia like '\"+str(dependencia)+\"';\"\n",
    "    cursor.execute(query)\n",
    "    rows=cursor.fetchall() \n",
    "    if len(rows)!=0:               \n",
    "       if rows[0][0] not in lista:\n",
    "            lista.append(rows[0][0])    \n",
    "            visited_node(rows[0][0],dependencia,lista,con)        \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#===========================Red por dependencia==============================\n",
    "#Crea una red por dependencia, en la base de datos hay cerca de 277\n",
    "def net_by_dependecia(con):\n",
    "    cursor=con.cursor()\n",
    "    edges=[]\n",
    "    #explorando el nodo superior aca cada nodo y trayendo el id\n",
    "    #todas las dependencias\n",
    "    query=\"Select distinct(id_dependencia) from estructura order by id_dependencia desc\"    \n",
    "    cursor.execute(query)\n",
    "    dependencias=cursor.fetchall()    \n",
    "    #id de de dependencias, lista\n",
    "    for dep in dependencias:\n",
    "        query=\"select id from dir_clean where id_dependencia= '\"+str(dep[0])+\"';\"\n",
    "        cursor.execute(query)\n",
    "        nodos=cursor.fetchall()\n",
    "        #Creamos el graph\n",
    "        G=nx.Graph()\n",
    "        a=[]\n",
    "        #Creamos los nodos para agregar a la red\n",
    "        for n in nodos:\n",
    "            #Funcion que genera los atributos para cada nodo\n",
    "            atributes=attributes(n[0],con) #genera el diccionario de atributos\n",
    "            #Agragando el id y los atributos\n",
    "            G.add_node(n[0],id_titulo=atributes['id_titulo'],id_institucion=atributes['id_institucion'],partido=atributes['partido'])#genera la lista de nodos y atributos                 \n",
    "                      \n",
    "        #obtenemos el cargo para cada nodo para poder buscar su superior\n",
    "        query=\"select id,id_cargos from dir_clean where id_dependencia= '\"+str(dep[0])+\"';\"\n",
    "        cursor.execute(query)\n",
    "        nodos=cursor.fetchall()    \n",
    "        #EDGES\n",
    "        #Ahora se generaran los edges\n",
    "        for n in nodos:\n",
    "            id_cargo_superior=who(n[1],dep[0],con)\n",
    "            #buscando al funcionario de cargo superior\n",
    "            query=\"select id from dir_clean where id_cargos='\"+str(id_cargo_superior)+\"' and id_dependencia ='\"+str(dep[0])+\"';\"\n",
    "            cursor.execute(query)\n",
    "            ans=cursor.fetchall()\n",
    "            if len(ans)>0:\n",
    "                #para el nodo si riene un superior se calcula el peso\n",
    "                wei=weight(attributes(n[0],con),attributes(ans[0][0],con))\n",
    "                #Ya que fue calculado el peso se agrega al edge\n",
    "                G.add_edge(n[0],ans[0][0], weight=wei)\n",
    "                #si no tiene un superior se agrega un edge a su mismo con peso 0\n",
    "            else:\n",
    "                G.add_edge(n[0],n[0], weight=0)\n",
    "                #edges.append([n[0],n[0]], weight=wi)        \n",
    "    #para cada dependencia se construye una red  \n",
    "    #DEVELOP GRAPH       \n",
    "        #agregando los atributos               \n",
    "        cursor.execute(\"select dependencia from estructura where id_dependencia like '\"+str(dep[0])+\"'\")\n",
    "        ans=cursor.fetchall()\n",
    "        nombre_dependencia=ans[0][0]\n",
    "        #llama la función de calcula las medidas\n",
    "        measure=measurements(G,con,nombre_dependencia.decode(encoding='UTF-8',errors='strict'),str(dep[0]))\n",
    "        \n",
    "    #clasificando las carreras, reduciendo la lista con dedupe y calculando la distancia al más cercano. \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===============Calculando peso para el edge =====================================\n",
    "def weight(list1,list2):\n",
    "    peso=0.5 #por pertenecer a la misma dependencia\n",
    "    if list1['id_titulo']==list2['id_titulo']:\n",
    "        peso=peso+0.15 #tienen la misma carrera(de hecho deberia ser mas fuerte si ademas estuvieron en la misma escuela)\n",
    "    if (list1['id_institucion']==list2['id_institucion']):\n",
    "        peso=peso+0.3    #estudiaron en la misma escuela\n",
    "    if (list1['partido']==list2['partido']):\n",
    "        peso=peso+0.05 # pertenecen al mismo partido, le reste importancia ya que no todos tienen afiliación política\n",
    "    \n",
    "    return peso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============Medidas==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============Calculo de medidas para la red===================================    \n",
    "def measurements(G,con,nombre,id):\n",
    "        #Configuración para el plot\n",
    "        \n",
    "        #plt.figure(figsize=(10,10))\n",
    "        #plt.axis('off')  \n",
    "        #plt.title(nombre)    \n",
    "        nx.draw_spring(G, with_labels=True)\n",
    "        nx.draw(G, with_labels=True)\n",
    "        #calcula centrality\n",
    "        central,top_de,top_clos,bet_top,top_de_c=centrality(G,con)\n",
    "        #calculo del linkedin prediction\n",
    "        linkedin,jackard,pref=linkedin_prediction(G)\n",
    "        #calculo de clustering y cliques\n",
    "        cliques,maximal,bipartite,graph_clique_number,component_connected_g=community(G,con,nombre,id)#AGREGUE ESTO    \n",
    "        \n",
    "        #Generando archivos JSON para salida de datos\n",
    "        path=os.getcwd()+\"/data/\"+str(id)+\".json\" \n",
    "        with open(path, 'w') as outfile1:\n",
    "            outfile1.write(json.dumps(json_graph.node_link_data(G)))\n",
    "            \n",
    "        path=os.getcwd()+\"/data/\"+str(id)+\"_centrality.json\" \n",
    "        with open(path, 'w') as outfile1:\n",
    "            outfile1.write(json.dumps(central))   \n",
    "                    \n",
    "        path=os.getcwd()+\"/data/\"+str(id)+\"_linkedin_prediction.json\" \n",
    "        with open(path, 'w') as outfile1:\n",
    "            outfile1.write(json.dumps(linkedin))\n",
    "        \n",
    "        path=os.getcwd()+\"/data/\"+str(id)+\"_all.csv\" \n",
    "\n",
    "        report(path,top_de,top_clos,bet_top,top_de_c,jackard,pref, cliques,maximal,bipartite,graph_clique_number,component_connected_g)\n",
    "\n",
    "\n",
    "        path=os.getcwd()+\"/data/\"+str(id)+\".gexf\"                \n",
    "        \n",
    "        #clustering(G,con,nombre,id)\n",
    "        #guardando la red con el id de la dependencia\n",
    "        nx.write_gexf(G, path,encoding='utf-8')       \n",
    "        path=os.getcwd()+\"/images/\"+str(id)\n",
    "        #ploteando\n",
    "        #plt.savefig(path)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#================Linkedin Prediction=======================================\n",
    "#Cuales son los nodos más probables a tener conexión en el futuro\n",
    "def linkedin_prediction(G):\n",
    "    #Link Prediction,Jaccard Coefficient\n",
    "    #Top 5 que tienen más probabilidad de conectarse\n",
    "    jackard=[]\n",
    "    preds_jc = nx.jaccard_coefficient(G)\n",
    "    pred_jc_dict = {}\n",
    "    for u, v, p in preds_jc:\n",
    "        pred_jc_dict[(u,v)] = p\n",
    "    Jaccard_Coefficient=[]\n",
    "    Jaccard_Coefficient_10=sorted(pred_jc_dict.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "    for c in Jaccard_Coefficient_10:\n",
    "        j={\"Nodes\":c[0],\"probability\":c[1]}\n",
    "        Jaccard_Coefficient.append(j)\n",
    "        jackard.append([c[0],c[1]])\n",
    "    \n",
    "    #Preferential attacment, top 5 más importante o reelevantes a conectarse\n",
    "    preds_pa = nx.preferential_attachment(G)\n",
    "    pref=[]\n",
    "    pred_pa_dict = {}\n",
    "    for u, v, p in preds_pa:\n",
    "        pred_pa_dict[(u,v)] = p\n",
    "    preferential=[]\n",
    "    preferential_10=sorted(pred_pa_dict.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "    for c in preferential_10:\n",
    "        j={\"Nodes\":c[0],\"measure\":c[1]}\n",
    "        preferential.append(j)\n",
    "        pref.append([c[0],c[1]])\n",
    "    dir={\"Jaccard_Coefficient\":Jaccard_Coefficient,\"preferential\":preferential}\n",
    "    return dir,jackard,pref\n",
    "\n",
    "    \n",
    "#==============================Centrality=======================================\n",
    "#Cuales son los nodos centrales en la red\n",
    "def centrality(G,con):\n",
    "    cursor=con.cursor()\n",
    "    #Top 5 de nodos centrales\n",
    "    centre=sorted(G.degree().items(), key=lambda x:x[1], reverse=True)[:5]    \n",
    "    top_degree=[]\n",
    "    top_de=[]\n",
    "    #Busca quienes son los nodos centrales en la base de datos\n",
    "    for c in centre:\n",
    "        query=\"select nombre,primer_apellido,segundo_apellido from dir_clean where id =\"+str(c[0])+\"\"\n",
    "        cursor.execute(query)\n",
    "        ans=cursor.fetchall()\n",
    "        top={\"nombre\":ans[0][0],\"primer_apellido\":ans[0][1],\"segundo_apellido\":ans[0][2],\"top\":c[1]}\n",
    "        top_degree.append(top)\n",
    "        top_de.append([ans[0][0],ans[0][1],ans[0][2],c[1]])\n",
    "    #calcula el Closeness centrality para la red\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    closeness=[]\n",
    "    #Top 5\n",
    "    closs_5= sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_clos=[]\n",
    "    for c in closs_5:\n",
    "        clo={\"id\":c[0],\"closeness\":c[1]}\n",
    "        closeness.append(clo)\n",
    "        top_clos.append([c[0],c[1]])\n",
    "    #Calcula el Betweeness centrality el top 5\n",
    "    betweeness_centrality = nx.betweenness_centrality(G)\n",
    "    betw_5=sorted(betweeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    betweeness_centrality=[]\n",
    "    bet_top=[]\n",
    "    for c in betw_5:\n",
    "        be={\"id\":c[0],\"betweeness\":c[1]}\n",
    "        betweeness_centrality.append(be)\n",
    "        bet_top.append([c[0],c[1]])\n",
    "    #Closnes\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    #top centrality degree \n",
    "    top_de_c=[]\n",
    "    top_degree_centrality=[]\n",
    "    top_degree_centrality_5=sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for c in top_degree_centrality_5:\n",
    "        t={\"id\":c[0],\"top_degree_centrality\":c[1]}\n",
    "        top_degree_centrality.append(t)    \n",
    "        top_de_c.append([c[0],c[1]])\n",
    "    #genera un diccionario para las medidas\n",
    "    dir={\"centrality\":top_degree,\"closeness\":closeness,\"betweeness_centrality\":betweeness_centrality,\"top_degree_centrality\":top_degree_centrality}\n",
    "    return dir,top_de,top_clos,bet_top,top_de_c\n",
    "\n",
    "#===================Calcula el grafo central===============================\n",
    "def centrality_graph_degree(G):\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    #plt.axis('on')  \n",
    "    deg=nx.degree(G)\n",
    "    #h=plt.hist(deg.values(),100)\n",
    "    #plt.loglog(h[1][1:],h[0])\n",
    "\n",
    "#=====================Elimina nodos con un threshold de grado==================\n",
    "#Si se desea eliminar nodos con poca conección\n",
    "def trim_degrees(g, degree=2):\n",
    "    g2=g.copy()\n",
    "    d=nx.degree(g2)\n",
    "    for n in g2.nodes():\n",
    "        if d[n]<=degree:\n",
    "            g2.remove_node(n)\n",
    "    return g2        \n",
    "#consulta los datos de cada nodo y genera un diccionario que será agregado como atributo a la red        \n",
    "\n",
    "    \n",
    "#==============================Attributes======================================\n",
    "#Consulta la base de datos para traer los atributos para cada nodo e incluirlos\n",
    "def attributes( id,con):    \n",
    "    cursor=con.cursor()    \n",
    "    query=\"select id_titulo,id_institucion from sep_ascii where id like '\"+str(id)+\"'\";\n",
    "    cursor.execute(query)\n",
    "    ans=cursor.fetchall()\n",
    "    #si existe la información\n",
    "    if len(ans)>0:\n",
    "        id_titulo=ans[0][0] #titulo profesional\n",
    "        id_institucion=ans[0][1] #escuela de prosedencia\n",
    "    else:\n",
    "        id_titulo=\"\"\n",
    "        id_institucion=\"\"\n",
    "    query=\"select partido from dir_clean where id=\"+str(id)+\"\";\n",
    "    cursor.execute(query)\n",
    "    ans=cursor.fetchall()\n",
    "    #consulta el partido\n",
    "    if len(ans)>0:\n",
    "        partido=ans[0][0]\n",
    "    else:\n",
    "        partido=\"\"\n",
    "    attribute={'id_titulo':str(id_titulo),'id_institucion':str(id_institucion),'partido':str(partido)}\n",
    "    \n",
    "    return attribute\n",
    "    \n",
    "    \n",
    "#==================================Quien es el nodo superior===============================\n",
    "#acorde al cargo se consulta en la tabla estructura quien es el cargo superior\n",
    "def who(id_cargos,dependencia,con):\n",
    "    cursor=con.cursor()\n",
    "    query=\"select id_cargo_superior from estructura where id_cargos like '\"+str(id_cargos)+\"' and id_dependencia like '\"+str(dependencia)+\"';\"\n",
    "    cursor.execute(query)\n",
    "    ans=cursor.fetchall()\n",
    "    return ans[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=================Calcula los clique de la red===================================\n",
    "# Calculando medidas de clique,community          \n",
    "def community(G,con,nombre,id):\n",
    "    #Based on the algorithm published by Bron & Kerbosch (1973) [R198] as adapated by Tomita,\n",
    "    #Tanaka and Takahashi (2006) [R199] and discussed in Cazals and Karande (2008) [R200]. \n",
    "    #The method essentially unrolls the recursion used in the references to avoid issues of recursion stack depth.\n",
    "    #This algorithm is not suitable for directed graphs.\n",
    "    #This algorithm ignores self-loops and parallel edges as clique is not conventionally defined with such edges.\n",
    "    #calcula todos los cliques\n",
    "    \n",
    "    cliques=list(nx.find_cliques(G))    \n",
    "    #  Create the maximal clique graph of a graph.\n",
    "    #Finds the maximal cliques and treats these as nodes. \n",
    "    #The nodes are connected if they have common members in the original graph. \n",
    "    #Theory has done a lot with clique graphs, but I haven’t seen much on maximal clique graphs.            \n",
    "    maximal=list(nx.make_max_clique_graph(G))\n",
    "    #This module provides functions and operations for bipartite graphs. \n",
    "    #Bipartite graphs B = (U, V, E) have two node sets U,V and edges in E \n",
    "    #that only connect nodes from opposite sets. It is common in the literature\n",
    "    #to use an spatial analogy referring to the two node sets as top and bottom nodes.\n",
    "    bipartite= list(nx.make_clique_bipartite(G, fpos=None, create_using=None, name=None))\n",
    "    #obtiene el número de cliques en la red\n",
    "    graph_clique_number=nx.graph_clique_number(G, cliques=None)\n",
    "    #Componentes conectados\n",
    "    component_connected_g=[]\n",
    "    compo=nx.connected_component_subgraphs(G)\n",
    "    for c in compo:\n",
    "        component_connected_g.append(list(c))\n",
    "        \n",
    "    return cliques,maximal,bipartite,graph_clique_number,component_connected_g\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===============================Genera Red con un archivo============================\n",
    "#Si se tiene una lista de ids de funcionarios se genera la red a partir de esta lista\n",
    "def develop_net_dependencia_with_FIlE(file,con):\n",
    "    cursor=con.cursor()\n",
    "    g = Graph()\n",
    "    edges=[]\n",
    "    vertices=[]\n",
    "    partidos=[]\n",
    "    nombres=[]\n",
    "    with open(file,'rb') as csvfile:\n",
    "        reader=csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            query=\"select id,nombre,primer_apellido,segundo_apellido from dir_clean where id_cargos like '\"+row[2]+\"' and id_dependencia like '\"+row[1]+\"';\"            \n",
    "            vertices.append(str(row[0]))                        \n",
    "            cursor.execute(query)\n",
    "            result=cursor.fetchall()            \n",
    "            for r in result:\n",
    "                #print r[0],r[1]\n",
    "                if row[0]!=r[0]:\n",
    "                    edges.append((str(row[0]),str(r[0])))\n",
    "                nombres.append(r[1])\n",
    "                query=\"select id from partidos_ascii where nombre like upper('\"+str(r[1])+\"') and apellido_paterno like upper('\"+r[2]+\"') and apellido_materno like upper('\"+r[3]+\"');\"\n",
    "                #print query\n",
    "                cursor.execute(query)\n",
    "                results=cursor.fetchall()\n",
    "                \n",
    "                if len(results)>0:\n",
    "                    query=\"select partido from partidos where id= \"+str(results[0][0])\n",
    "                    cursor.execute(query)\n",
    "                    resu=cursor.fetchall() \n",
    "                    if len(resu)>0:\n",
    "                        partidos.append(resu[0][0])\n",
    "                else:\n",
    "                    #print \"no\"\n",
    "                    partidos.append(\"No\")\n",
    "            #si no esta en la base de datos\n",
    "            if len(result)==0:\n",
    "                print (\"No encontre este\",row[0])\n",
    "            \n",
    "     \n",
    "    g.add_vertices(vertices)        \n",
    "    g.add_edges(edges)\n",
    "    g.vs\n",
    "    #g.vs[\"nombres\"]=vertices\n",
    "    g.vs[\"partidos\"]=partidos\n",
    "    layout = g.layout(\"fr\")\n",
    "    visual_style = {}\n",
    "    visual_style[\"layout\"] = layout\n",
    "    visual_style[\"vertex_size\"] = 25\n",
    "    visual_style[\"label_size\"]=9\n",
    "    visual_style[\"vertex_label\"] =g.vs[\"label\"]=vertices\n",
    "    n=[\"PRI\",\"PAN\",\"PRD\",\"No\",\"MCI\",\"NA\",\"MOR\",\"PVE\"]\n",
    "    c=[\"red\",\"BLUE\",\"YELLOW\",\"GRAY\",\"ORANGE\",\"TURQUOISE\",\"BROWN\",\"green\"]\n",
    "    visual_style[\"legend\"]=[1, 95, n,c]\n",
    "    color_dict={\"PRI\":\"red\",\"PAN\":\"BLUE\",\"PRD\":\"YELLOW\",\"No\":\"GRAY\",\"MCI\":\"ORANGE\",\"NA\":\"TURQUOISE\",\"NA.\":\"TURQUOISE\",\"MOR\":\"BROWN\",\"PVE\":\"green\"}\n",
    "   \n",
    "    #visual_style[\"vertex_label\"] = g.vertices\n",
    "    visual_style[\"vertex_color\"] = [color_dict[partido] for partido in g.vs[\"partidos\"]]\n",
    "    visual_style[\"bbox\"] = (1000, 1000)\n",
    "    plot(g, **visual_style)\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==================CSV que unifica las medidas================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(path,top_de,top_clos,bet_top,top_de_c,jackard,pref,cliques,maximal,bipartite,graph_clique_number,component_connected_g):             \n",
    "        with open(path, \"wb\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"*Análisis de Redes*\"])\n",
    "            writer.writerow([\"================Centrality(Elite)===========\"])\n",
    "            writer.writerow([\"*Top degree centrality*\"])\n",
    "            writer.writerows(top_de)\n",
    "            \n",
    "            writer.writerow([\"*Top closeness*\"])\n",
    "            writer.writerows(top_clos)\n",
    "            \n",
    "            writer.writerow([\"*Top Betweetness*\"])\n",
    "            writer.writerows(bet_top)\n",
    "            \n",
    "            writer.writerow([\"*Top degree centrality*\"])\n",
    "            writer.writerows(top_de_c)\n",
    "            \n",
    "            \n",
    "            writer.writerow([\"==================Community================\"])\n",
    "            writer.writerow([\"*Cliques*\"])\n",
    "            writer.writerows(cliques)\n",
    "            writer.writerow([\"*Maximal clique*\"])\n",
    "\n",
    "            writer.writerow(maximal)\n",
    "            writer.writerow([\"*Bipartite clique*\"])\n",
    "            writer.writerow(bipartite)\n",
    "            writer.writerow([\"*Graph clique number*\"])\n",
    "            \n",
    "            writer.writerow([graph_clique_number])\n",
    "            writer.writerow([\"*Component connected Graph*\"])\n",
    "            writer.writerows(component_connected_g)\n",
    "            \n",
    "            writer.writerow([\"==================Prediction================\"])\n",
    "            writer.writerow([\"*Linkedin Prediction*\"])\n",
    "            writer.writerow([\"*Jackard*\"])\n",
    "            writer.writerows(jackard)\n",
    "            writer.writerow([\"*Preferencial*\"])\n",
    "            writer.writerows(pref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
